
### GET haiku with default params
GET {{host}}/chat/haiku

> {%
    client.test("Status", function () {
        client.assert(response.status === 200, "Unexpected response status");
    });
%}

### GET haiku with params
GET {{host}}/chat/haiku?style=crazy&topic=Trump

> {%
    client.test("Status", function () {
        client.assert(response.status === 200, "Unexpected response status");
    });
%}

### GET response from internal LLM like ollama with a fact
POST {{host}}/chat/llm?provider=OLLAMA
Content-Type: application/json

{
  "prompt": "I'm the president of France and my name is Macron",
  "style": "end answer with: Hope answer is correct!",
  "chat_id": "president-id"
}

### GET response from internal LLM like ollama
POST {{host}}/chat/llm?provider=OLLAMA
Content-Type: application/json

{
  "prompt": "who am I",
  "style": "end answer with: Hope answer is correct!",
  "chat_id": "president-id"
}

> {%
    client.test("Status", function () {
        client.assert(response.status === 200, "Unexpected response status");
    });
%}

### GET response from external LLM Anthropic
POST {{host}}/chat/llm?provider=ANTHROPIC
Content-Type: application/json

{
  "prompt": "Are Trump and Musk still BFF friends",
  "style": "answer with max 10 word and end answer with Presently!"
}

### GET response from external LLM OpenAi/chatGpt
POST {{host}}/chat/llm?provider=OPENAI
Content-Type: application/json

{
  "prompt": "Are Trump and Musk still best friends",
  "style": "answer with max 10 word and end answer with Presently!"
}

### GET response using memory
POST {{host}}/chat/llm?provider=OPENAI
Content-Type: application/json

{
  "prompt": "My name is Max and I'm the brother of Måns",
  "style": "end answer with: Got it!",
  "chat_id": "me-id"
}

### GET response relating to previous question
POST {{host}}/chat/llm?provider=OPENAI
Content-Type: application/json

{
  "prompt": "Tell everything about Måns",
  "style": "end answer with: I recall!",
  "chat_id": "me-id"
}

### GET response relating to previous question with correction
POST {{host}}/chat/llm?provider=OPENAI
Content-Type: application/json

{
  "prompt": "You forgot to mention that Måns has a brother Max",
  "style": "end answer with: I recall!",
  "chat_id": "me-id"
}

### GET response with complex relations
POST {{host}}/chat/llm?provider=OPENAI
Content-Type: application/json

{
  "prompt": "I, Tom, have a half brother Rick that has a half brother named John. How many brothers does John have",
  "style": "end answer with: Ok!"
}

### GET response with correction
POST {{host}}/chat/llm?provider=OPENAI
Content-Type: application/json

{
  "prompt": "Hope you understand the three different cases: 1. John and I have the same two parents. 2. John and I share one parent. 3. John and I have no parents in common.",
  "style": "end answer with: I got it!"
}

> {%
    client.test("Status", function () {
        client.assert(response.status === 200, "Unexpected response status");
    });
%}

### GET LLM response from docker
POST {{host}}/chat/llm?provider=DOCKER
Content-Type: application/json

{
  "prompt": "who is president in France",
  "style": "end answer with: Hope answer is ok!"
}

> {%
    client.test("Status", function () {
        client.assert(response.status === 200, "Unexpected response status");
    });
%}

### GET response from all LLMs like ollama, docker, openai, anthropic
POST {{host}}/chat/combine
Content-Type: application/json

{
  "prompt": "Are Trump and Musk in the Epstein files",
  "style": "answer with max 10 word and end answer with Presently!"
}


### GET one or several responses from request
GET {{host}}/chat/request/2466e808-a176-4a7f-a09b-2d461ef033b5

### GET several responses from several request with same chatId
GET {{host}}/chat/chat/ping-chat-service-status

### GET status of all LLMs
GET {{host}}/chat/status


### Info
GET {{host}}/actuator/info

### Prometheus
GET {{host}}/actuator/prometheus

### Health
GET {{host}}/actuator/health

### Liveness
GET {{host}}/actuator/health/liveness

### Readiness
GET {{host}}/actuator/health/readiness
