
### Create haiku with default params
POST {{host}}/chat/provider/haiku?provider=DOCKER

> {%
    client.test("Status", function () {
        client.assert(response.status === 200, "Unexpected response status");
    });
%}

### Create haiku with params
POST {{host}}/chat/provider/haiku?provider=DOCKER&style=crazy&topic=Trump

> {%
    client.test("Status", function () {
        client.assert(response.status === 200, "Unexpected response status");
    });
%}

### Create llmCompletion from internal LLM like ollama with a fact
POST {{host}}/chat/provider/prompt
Content-Type: application/json

{
  "prompt": "I'm the president of France and my name is Macron",
  "style": "end answer with: Hope answer is correct!",
  "chat_id": "president-id",
  "llm_provider": "OLLAMA"
}

### invalid input
POST {{host}}/chat/provider/prompt
Content-Type: application/json

{
  "prompt": "I",
  "chat_id": "president-id",
  "llm_provider": "DOCKER",
  "style": "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"
}

### Create llmCompletion from internal LLM like ollama
POST {{host}}/chat/provider/prompt
Content-Type: application/json

{
  "prompt": "who am I",
  "style": "end answer with: Hope answer is correct!",
  "chat_id": "president-id",
  "llm_provider": "OLLAMA"
}

> {%
    client.test("Status", function () {
        client.assert(response.status === 200, "Unexpected response status");
    });
%}

### Create llmCompletion from external LLM Anthropic
POST {{host}}/chat/provider/prompt
Content-Type: application/json

{
  "prompt": "Are Trump and Musk still BFF friends",
  "style": "answer with max 10 word and end answer with Presently!",
  "llm_provider": "ANTHROPIC"
}

### Create llmCompletion from external LLM OpenAi/chatGpt
POST {{host}}/chat/provider/prompt
Content-Type: application/json

{
  "prompt": "Are Trump and Musk still best friends",
  "style": "answer with max 10 word and end answer with Presently!",
  "llm_provider": "OPENAI"
}

### Create llmCompletion from Gemini
POST {{host}}/chat/provider/prompt
Content-Type: application/json

{
  "prompt": "What is the relation between Hassabis, Hinton and LeCun",
  "style": "answer with max 200 words",
  "llm_provider": "GEMINI"
}

### Create llmCompletion using chat-id for a given provider
POST {{host}}/chat/provider/prompt
Content-Type: application/json

{
  "prompt": "My name is Max and I'm the brother of Måns",
  "style": "end answer with: Got it!",
  "chat_id": "me-id",
  "llm_provider": "OPENAI"
}

### Create llmCompletion relating to previous question
POST {{host}}/chat/provider/prompt
Content-Type: application/json

{
  "prompt": "Tell everything about Måns",
  "style": "end answer with: I recall!",
  "chat_id": "me-id",
  "llm_provider": "OPENAI"
}

### Create llmCompletion relating to previous question with correction
POST {{host}}/chat/provider/prompt
Content-Type: application/json

{
  "prompt": "You forgot to mention that Måns has a brother Max",
  "style": "end answer with: I recall!",
  "chat_id": "me-id",
  "llm_provider": "OPENAI"
}

### Create llmCompletion with complex relations
POST {{host}}/chat/provider/prompt
Content-Type: application/json

{
  "prompt": "I, Tom, have a half brother Rick that has a half brother named John. How many brothers does John have",
  "style": "end answer with: Ok!",
  "llm_provider": "OPENAI"
}

### Create llmCompletion with correction
POST {{host}}/chat/provider/prompt
Content-Type: application/json

{
  "prompt": "Hope you understand the three different cases: 1. John and I have the same two parents. 2. John and I share one parent. 3. John and I have no parents in common.",
  "style": "end answer with: I got it!",
  "llm_provider": "OPENAI"
}

> {%
    client.test("Status", function () {
        client.assert(response.status === 200, "Unexpected response status");
    });
%}

### Create LLM llmCompletion from docker
POST {{host}}/chat/provider/prompt
Content-Type: application/json

{
  "prompt": "who is president in France",
  "style": "end answer with: Hope answer is ok!",
  "llm_provider": "DOCKER"
}

> {%
    client.test("Status", function () {
        client.assert(response.status === 200, "Unexpected response status");
    });
%}

### Create llmCompletion from given providers to test Tooling
POST {{host}}/chat/providers/prompt
Content-Type: application/json

{
  "prompt": "Explain superposition in quantum mechanics?",
  "style": "elaborate with max 30 words",
  "chat_id": "superposition",
  "llm_providers": ["GEMINI","ANTHROPIC","OPENAI"]
}

### Get chat history for a given chat id and provider
GET {{host}}/chat/history?chat_id=superposition&provider=ANTHROPIC

> {%
    client.test("Status", function () {
        client.assert(response.status === 200, "Unexpected response status");
    });
%}

### Clear chat history for a given chat id and provider
DELETE {{host}}/chat/history/clear?chat_id=superposition&provider=ANTHROPIC

> {%
    client.test("Status", function () {
        client.assert(response.status === 204, "Unexpected response status");
    });
%}

### Get chat history for a given chat id and provider, now returns 404
GET {{host}}/chat/history?chat_id=superposition&provider=ANTHROPIC

> {%
    client.test("Status", function () {
        client.assert(response.status === 404, "Unexpected response status");
    });
%}

### Check that ANTHROPIC does not understand the question after clearing it's history
POST {{host}}/chat/providers/prompt
Content-Type: application/json

{
  "prompt": "Who coined the term?",
  "style": "elaborate with max 30 words",
  "chat_id": "superposition",
  "llm_providers": ["GEMINI","ANTHROPIC","OPENAI"]
}

> {%
    client.test("Status", function () {
        client.assert(response.status === 200, "Unexpected response status");
    });
%}

### Create llmCompletion from all LLMs like ollama, docker, openai, anthropic
POST {{host}}/chat/providers/prompt
Content-Type: application/json

{
  "prompt": "Are Trump and Musk in the Epstein files",
  "style": "answer with max 10 word and end answer with Presently!",
  "llm_providers": ["OLLAMA", "DOCKER", "OPENAI", "ANTHROPIC","GEMINI"]
}

> {%
    client.test("Status", function () {
        client.assert(response.status === 200, "Unexpected response status");
    });
%}


### GET one or several completions from a given llmPrompt id
GET {{host}}/chat/prompt/52e7c62c-68d7-4edf-b1a8-a964aee3b447

### GET one or several completions from a given text in a previous prompt (case sensitive)
GET {{host}}/chat/prompt/contains/Epstein

### GET one or several interactions from a given text in a previous completion (case sensitive)
GET {{host}}/chat/completion/contains/Trump

### GET one completion from a given llmCompletion id
GET {{host}}/chat/completion/27e957a9-4a6b-4126-815a-6d06d043ef92

### GET several completions from several llmPrompt with same chatId
GET {{host}}/chat/chat/ping-chat-service-status

### GET several completions from several llmPrompt with same chatId
GET {{host}}/chat/chat/myId

### GET status of all LLMs
GET {{host}}/chat/providers/status


### Info
GET {{host}}/actuator/info

### Prometheus
GET {{host}}/actuator/prometheus

### Health
GET {{host}}/actuator/health

### Liveness
GET {{host}}/actuator/health/liveness

### Readiness
GET {{host}}/actuator/health/readiness
